{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-6OiYMwqKR1"
   },
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25441,
     "status": "ok",
     "timestamp": 1582006923552,
     "user": {
      "displayName": "Martin Vielvoye",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAyYaQ-3jMfwW1KswvADS02u_w33tBF8mTJjs-ClQ=s64",
      "userId": "09542053193333209680"
     },
     "user_tz": -60
    },
    "id": "6RZf7rt1qhXs",
    "outputId": "83d4c984-2ab3-4f5f-8955-8e227de0e417"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/martinvielvoye/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/martinvielvoye/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/martinvielvoye/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import flair\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "import transformers\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix, andrews_curves\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APYYIrX6qPDS"
   },
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j_oxoz17oUTB"
   },
   "outputs": [],
   "source": [
    "def y_output(x):\n",
    "    tot_list = np.zeros(len(y_tags))\n",
    "    for element in x:\n",
    "        if(element in y_tags):\n",
    "            tot_list[y_tags.tolist().index(element)] += 1\n",
    "    return tot_list\n",
    "\n",
    "def totListReg(x, totList):\n",
    "    regex = r\"\\<(.*?)\\>\"\n",
    "    #print(x)\n",
    "    matches = re.finditer(regex, x,re.MULTILINE)\n",
    "    tagList = []\n",
    "    for matchNum, match in enumerate(matches, start = 1):\n",
    "        #print (\"Match {matchNum} was found at {start}-{end}: {match}\".format(matchNum = matchNum, start = match.start(), end = match.end(), match = match.group()))\n",
    "    \n",
    "        for groupNum in range(0, len(match.groups())):\n",
    "            groupNum = groupNum + 1\n",
    "            tagList.append(match.group(groupNum))\n",
    "            #print (\"Group {groupNum} found at {start}-{end}: {group}\".format(groupNum = groupNum, start = match.start(groupNum), end = match.end(groupNum), group = match.group(groupNum)))\n",
    "    for values in tagList:\n",
    "        totList.append(values)\n",
    "    #return tagList if match else None\n",
    "    return tagList\n",
    "\n",
    "def listReg(x):\n",
    "    regex = r\"\\<(.*?)\\>\"\n",
    "    #print(x)\n",
    "    matches = re.finditer(regex, x,re.MULTILINE)\n",
    "    tagList = []\n",
    "    for matchNum, match in enumerate(matches, start = 1):\n",
    "        #print (\"Match {matchNum} was found at {start}-{end}: {match}\".format(matchNum = matchNum, start = match.start(), end = match.end(), match = match.group()))\n",
    "    \n",
    "        for groupNum in range(0, len(match.groups())):\n",
    "            groupNum = groupNum + 1\n",
    "            tagList.append(match.group(groupNum))\n",
    "            #print (\"Group {groupNum} found at {start}-{end}: {group}\".format(groupNum = groupNum, start = match.start(groupNum), end = match.end(groupNum), group = match.group(groupNum)))\n",
    "    #return tagList if match else None\n",
    "    return tagList\n",
    "\n",
    "def InputTransf(x):\n",
    "    html = x[\"Body\"]\n",
    "    raw = BeautifulSoup(html, \"html.parser\").get_text()\n",
    "\n",
    "    tokens = word_tokenize(raw)\n",
    "    tokens = [s + \" \" for s in tokens]\n",
    "    if(type(x[\"Title\"]) == np.float): \n",
    "        fullText = (''.join(tokens[:]))\n",
    "    else:\n",
    "        fullText = (x[\"Title\"] + \". \" + ''.join(tokens[:]))\n",
    "    return fullText\n",
    "    \n",
    "    #text = nltk.Text(tokens)\n",
    "    #text\n",
    "\n",
    "def CleanedInputStem(x):\n",
    "    sw = set()\n",
    "    sw.update(tuple(nltk.corpus.stopwords.words('english')))\n",
    "    \n",
    "    tokens = word_tokenize(x)\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    corpora = [stemmer.stem(w) for w in tokens if not w in list(sw)]\n",
    "    #corpora = [w for w in tokens if not w in list(sw)]\n",
    "    #corpora = [s + \" \" for s in corpora]\n",
    "    #print(''.join(corpora[:]))\n",
    "\n",
    "    return corpora\n",
    "\n",
    "def SemiCleanedInput(x):\n",
    "    sw = set()\n",
    "    sw.update(tuple(nltk.corpus.stopwords.words('english')))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokens = word_tokenize(x)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    #corpora = [lemmatizer.lemmatize(w.lower()) for w in tokens if not w in list(sw)]\n",
    "    #corpora = list(set(corpora))\n",
    "    corpora = list(dict.fromkeys(tokens))\n",
    "    #print(corpora)\n",
    "    corpora = \" \".join(corpora[:])\n",
    "    #print(corpora)\n",
    "\n",
    "\n",
    "    #corpora = [w for w in tokens if not w in list(sw)]\n",
    "    #corpora = [s + \" \" for s in corpora]\n",
    "    #print(''.join(corpora[:]))\n",
    "\n",
    "    return corpora\n",
    "\n",
    "def CleanedInputLemma(x):\n",
    "    sw = set()\n",
    "    sw.update(tuple(nltk.corpus.stopwords.words('english')))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokens = word_tokenize(x)\n",
    "    tags2 = nltk.pos_tag(tokens)\n",
    "    tokens = [word.lower() for word,pos in tags2 if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]\n",
    "    #corpora = [lemmatizer.lemmatize(w.lower()) for w in tokens if not w in list(sw)]\n",
    "    #corpora = list(set(corpora))\n",
    "    corpora = list(dict.fromkeys(tokens))\n",
    "    \n",
    "\n",
    "    #corpora = [w for w in tokens if not w in list(sw)]\n",
    "    #corpora = [s + \" \" for s in corpora]\n",
    "    #print(''.join(corpora[:]))\n",
    "\n",
    "    return corpora\n",
    "    \n",
    "def inputToArray(x, tot_array = []):\n",
    "    \n",
    "    vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "    X2 = vectorizer2.fit_transform([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9auUvdw-stO6"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./QueryResults.csv\",\n",
    "                           engine=\"python\",\n",
    "                           sep=',',\n",
    "                           decimal='.')\n",
    "\n",
    "emb_data = pd.read_csv(\"./Top10000Scores.csv\",\n",
    "                           engine=\"python\",\n",
    "                           sep=',',\n",
    "                           decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hYJ67ybkrYB"
   },
   "outputs": [],
   "source": [
    "featData = data[\"Tags\"].apply(listReg)\n",
    "featData = pd.DataFrame(featData, index = featData.index)\n",
    "featData[\"Input\"] = data[[\"Title\", \"Body\"]].apply(InputTransf, axis = 1)\n",
    "featData[\"Clean_Input\"] = featData[\"Input\"].apply(CleanedInputLemma)\n",
    "featData[\"Semiclean_Input\"] = featData[\"Input\"].apply(SemiCleanedInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load elmo_train_new\n",
    "pickle_in = open(\"./elmo_train_03032019.pickle\", \"rb\")\n",
    "elmo_train_new = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "all_train = []\n",
    "for i in range(np.int(len(elmo_train_new)/2)):\n",
    "  all_train.append(elmo_train_new[i*2])\n",
    "\n",
    "emb_train = np.concatenate(all_train, axis = 0)\n",
    "len_train = len(emb_train)\n",
    "#Need to clean \"train[label]\" for actual labels\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(emb_train, \n",
    "                                                  featData[\"y_output\"][:len_train].tolist(),  \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1)\n",
    "len_act = len(xtrain)\n",
    "y_sparse_train = featData[\"y_output\"].values.tolist()[:len_act]\n",
    "y_sparse_test = featData[\"y_output\"].values.tolist()[len_act:len_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4ZZSmjtoNZ_"
   },
   "outputs": [],
   "source": [
    "totList = []\n",
    "data[\"Tags\"].apply(totListReg, args=(totList,))\n",
    "allTags = pd.DataFrame(totList, columns = [\"uniTags\"])\n",
    "allTags[\"uniTags\"].value_counts()\n",
    "\n",
    "y_tags = allTags[\"uniTags\"].value_counts().head(150).index.values\n",
    "featData[\"y_output\"] = featData[\"Tags\"].apply(y_output)\n",
    "\n",
    "clean_array = featData[\"Clean_Input\"].values.tolist()\n",
    "temp = []\n",
    "for strin in clean_array:\n",
    "    strin = [s + \" \" for s in strin]\n",
    "    temp.append(''.join(strin))\n",
    "clean_array = temp\n",
    "\n",
    "x_train = clean_array[:42500]\n",
    "x_test = clean_array[42500:]\n",
    "\n",
    "clean_array_tag = featData[\"Tags\"].values.tolist()\n",
    "temp = []\n",
    "for strin in clean_array_tag:\n",
    "    strin = [s for s in strin]\n",
    "    temp.append(' '.join(strin))\n",
    "clean_array_tag = temp\n",
    "\n",
    "y_clean_train = clean_array_tag[:42500]\n",
    "y_clean_test = clean_array_tag[42500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_data[\"Input\"] = emb_data[[\"Title\", \"Body\"]].apply(InputTransf, axis = 1)\n",
    "emb_data[\"Clean_Input\"] = emb_data[\"Input\"].apply(CleanedInputLemma)\n",
    "emb_data[\"SemiClean_Input\"] = emb_data[\"Input\"].apply(SemiCleanedInput2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_emb_array_clean = emb_data[\"Clean_Input\"].values.tolist()\n",
    "temp = []\n",
    "for strin in clean_emb_array_clean:\n",
    "    strin = [s + \" \" for s in strin]\n",
    "    temp.append(''.join(strin))\n",
    "clean_emb_array_clean = temp\n",
    " \n",
    "clean_emb_array = emb_data[\"SemiClean_Input\"].values.tolist()\n",
    "temp = []\n",
    "for strin in clean_emb_array:\n",
    "    strin = [s for s in strin]\n",
    "    temp.append(''.join(strin))\n",
    "clean_emb_array = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_tok = []\n",
    "for sentences in clean_emb_array:\n",
    "    tokens = word_tokenize(sentences)\n",
    "    for words in tokens:\n",
    "        voc_tok.append(words)\n",
    "print(len(voc_tok))\n",
    "voc_tok = list(set(voc_tok))\n",
    "\n",
    "print(len(voc_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_data[\"Input\"] = emb_data[\"Input\"].apply(lambda x: word_tokenize(x.lower()))\n",
    "emb_data[[\"Input\", \"Clean_Input\", \"SemiClean_Input\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3r0aTZSkxO9"
   },
   "source": [
    "# Data Handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8227,
     "status": "ok",
     "timestamp": 1581930368288,
     "user": {
      "displayName": "Martin Vielvoye",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAyYaQ-3jMfwW1KswvADS02u_w33tBF8mTJjs-ClQ=s64",
      "userId": "09542053193333209680"
     },
     "user_tz": -60
    },
    "id": "-bAaWty1b02g",
    "outputId": "cdc238b7-e6e9-46c9-ed31-8bdef55b0ca5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#To make tf 2.0 compatible with tf1.0 code, we disable the tf2.0 functionalities\n",
    "#tf.disable_eager_execution()\n",
    "import tensorflow_hub as hub\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "#hub.disable_eager_execution()\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UtFLf1v3ILDM"
   },
   "source": [
    "```\n",
    "# just a random sentence\n",
    "x = [\"Roasted ants are a popular snack in Columbia\"]\n",
    "\n",
    "# Extract ELMo features \n",
    "embeddings = elmo(x, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "\n",
    "embeddings.shape\n",
    "```\n",
    "The output is a 3 dimensional tensor of shape (1, 8, 1024):\n",
    "\n",
    "The first dimension of this tensor represents the number of training samples. This is 1 in our case\n",
    "The second dimension represents the maximum length of the longest string in the input list of strings. Since we have only 1 string in our input list, the size of the 2nd dimension is equal to the length of the string – 8\n",
    "The third dimension is equal to the length of the ELMo vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I1FpZP22IF_p"
   },
   "outputs": [],
   "source": [
    "class Elmo_emb:\n",
    "  def __init__(self, ite):\n",
    "    self.ite = ite\n",
    "\n",
    "  def elmo_vectors(self, x):\n",
    "    embeddings = elmo(x, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    if(self.ite%5 == 0): print(self.ite, \"out of\", len(list_train))\n",
    "    self.ite += 1\n",
    "    with tf.Session() as sess:\n",
    "      sess.run(tf.global_variables_initializer())\n",
    "      sess.run(tf.tables_initializer())\n",
    "      # return average of ELMo features\n",
    "      return (sess.run(tf.reduce_mean(embeddings,1)), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EVEIM1cDIcJ9",
    "outputId": "76c3eba2-7272-43d4-e7be-948c9c8ce742"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c4d7d34bc4e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0melmo1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElmo_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0melmo_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0melmo1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melmo_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'list_train' is not defined"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "elmo1 = Elmo_emb(0)\n",
    "elmo_train= [elmo1.elmo_vectors(tf.convert_to_tensor(x)) for x in list_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 970,
     "status": "error",
     "timestamp": 1582002854422,
     "user": {
      "displayName": "Martin Vielvoye",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAyYaQ-3jMfwW1KswvADS02u_w33tBF8mTJjs-ClQ=s64",
      "userId": "09542053193333209680"
     },
     "user_tz": -60
    },
    "id": "Uu9ezuZ0RYlg",
    "outputId": "5d85ea9e-9fe7-41a0-9d2c-50c983e0e74f"
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic Tensor (module_apply_tokens/aggregation/mul_3:0) to a numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-ff72df3a1a76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0melmo_train_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#elmo_test_new = np.concatenate(elmo_test, axis = 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    726\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m     raise NotImplementedError(\"Cannot convert a symbolic Tensor ({}) to a numpy\"\n\u001b[0;32m--> 728\u001b[0;31m                               \" array.\".format(self.name))\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (module_apply_tokens/aggregation/mul_3:0) to a numpy array."
     ]
    }
   ],
   "source": [
    "elmo_train_new = np.concatenate([embeddings], axis = 0)\n",
    "#elmo_test_new = np.concatenate(elmo_test, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJwraObAgVJm"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle _thread.RLock objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-191-da7fc46a79e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# save elmo_train_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpickle_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./elmo_train_03032019.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpickle_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# save elmo_train_new\n",
    "pickle_out = open(\"./elmo_train_03032019.pickle\",\"wb\")\n",
    "pickle.dump(embeddings, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "# save elmo_test_new\n",
    "#pickle_out = open(\"/content/drive/My Drive/elmo_test_03032019.pickle\",\"wb\")\n",
    "#pickle.dump(elmo_test_new, pickle_out)\n",
    "#pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-sOG8gGsgnHg"
   },
   "outputs": [],
   "source": [
    "# load elmo_train_new\n",
    "#pickle_in = open(\"./elmo_train_03032019.pickle\", \"rb\")\n",
    "#elmo_train_new = pickle.load(pickle_in)\n",
    "import pickle\n",
    "\n",
    "# load elmo_train_new\n",
    "pickle_in = open(\"./elmo_train_03032019.pickle\", \"rb\")\n",
    "elmo_train_new = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "# load elmo_train_new\n",
    "#pickle_in = open(\"/content/drive/My Drive/elmo_test_03032019.pickle\", \"rb\")\n",
    "#elmo_test_new = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train = []\n",
    "for i in range(np.int(len(elmo_train_new)/2)):\n",
    "  all_train.append(elmo_train_new[i*2])\n",
    "\n",
    "emb_train = np.concatenate(all_train, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jO72lM2Ohoer"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Need to clean \"train[label]\" for actual labels\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(emb_train, \n",
    "                                                  featData[\"y_output\"][:len_train].tolist(),  \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1)\n",
    "len_act = len(xtrain)\n",
    "y_sparse_train = featData[\"y_output\"].values.tolist()[:len_act]\n",
    "y_sparse_test = featData[\"y_output\"].values.tolist()[len_act:len_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1581789043630,
     "user": {
      "displayName": "Martin Vielvoye",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAyYaQ-3jMfwW1KswvADS02u_w33tBF8mTJjs-ClQ=s64",
      "userId": "09542053193333209680"
     },
     "user_tz": -60
    },
    "id": "RIqXtbxpsMz7",
    "outputId": "427ba607-54ff-4042-b726-b9343a9f14ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1903,)"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(yvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(xtrain, y_sparse_train)\n",
    "\n",
    "predict_neigh = neigh.predict(xvalid)\n",
    "f1_scor = f1_score(y_sparse_test, predict_neigh, average='micro')\n",
    "f1_scor_2 = f1_score(y_sparse_test, predict_neigh, average='weighted')\n",
    "\n",
    "print(f1_scor, f1_scor_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if word is ' ': \n",
    "            pass\n",
    "        #if wv.vocab[word] is None: \n",
    "        #    print('Found One 2')\n",
    "        #if wv.vectors[wv.vocab[word].index] is None: \n",
    "        #    print(\"Found One 3\")\n",
    "        else:\n",
    "            if isinstance(word, np.ndarray):\n",
    "                mean.append(word)\n",
    "\n",
    "            elif word in wv.vocab:\n",
    "                #print(\"word\", word)\n",
    "                #print(\"wv.vocab[word]\", wv.vocab[word])\n",
    "                #print(\"wv.vocab[word].index\", wv.vocab[word].index)\n",
    "                #print()\n",
    "                mean.append(wv.vectors[wv.vocab[word].index])\n",
    "                all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        #logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(150,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, review) for review in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(emb_data[\"SemiClean_Input\"].tolist(), size=150, window = 10, min_count = 2, workers = 10)  # an empty model, no training yet\n",
    "model.train(clean_emb_array_clean, total_examples = len(clean_emb_array_clean), epochs = 10)  # can be a non-repeatable, 1-pass generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_word_average = word_averaging_list(model.wv,x_train)\n",
    "print('part 2')\n",
    "X_test_word_average = word_averaging_list(model.wv,x_test)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_w2v, xvalid_w2v, ytrain_w2v, yvalid_w2v = train_test_split(X_train_word_average, \n",
    "                                                  featData[\"y_output\"][:len_train].tolist(),  \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1)\n",
    "len_act = len(xtrain)\n",
    "y_sparse_train_w2v = featData[\"y_output\"].values.tolist()[:len_act]\n",
    "y_sparse_test_w2v = featData[\"y_output\"].values.tolist()[len_act:len_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(xtrain_w2v, y_sparse_train_w2v)\n",
    "\n",
    "predict_neigh = neigh.predict(xvalid_w2v)\n",
    "f1_scor = f1_score(y_sparse_test, predict_neigh, average='micro')\n",
    "f1_scor_2 = f1_score(y_sparse_test_w2v, predict_neigh, average='weighted')\n",
    "\n",
    "print(f1_scor, f1_scor_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rTDAjIzmxD-h"
   },
   "source": [
    "# ELMo FineTuning (no-TPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.15\n",
      "  Downloading tensorflow-1.15.0-cp37-cp37m-macosx_10_11_x86_64.whl (124.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 124.0 MB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (0.1.7)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.14.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (3.11.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (0.8.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.0)\n",
      "Collecting tensorflow-estimator==1.15.1\n",
      "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[K     |████████████████████████████████| 503 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.16.4)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.2)\n",
      "Collecting tensorboard<1.16.0,>=1.15.0\n",
      "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.11.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (0.33.6)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (0.8.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (3.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.24.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow==1.15) (45.0.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (0.16.0)\n",
      "\u001b[31mERROR: tensorflow-cpu 2.1.0 has requirement tensorboard<2.2.0,>=2.1.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-cpu 2.1.0 has requirement tensorflow-estimator<2.2.0,>=2.1.0rc0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.1.0\n",
      "    Uninstalling tensorflow-estimator-2.1.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.1.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.1.0\n",
      "    Uninstalling tensorboard-2.1.0:\n",
      "      Successfully uninstalled tensorboard-2.1.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.1.0\n",
      "    Uninstalling tensorflow-2.1.0:\n",
      "      Successfully uninstalled tensorflow-2.1.0\n",
      "Successfully installed tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n",
      "2.2.4-tf\n",
      "0.7.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)\n",
    "print(hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "INFO:absl:Using /var/folders/79/yszgmhws5xl_3lvz88nm204h0000gn/T/tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import our dependencies\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "import keras\n",
    "from keras import backend as K\n",
    "#import keras.layers as layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten,Input, Layer\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Lambda\n",
    "import numpy as np\n",
    "\n",
    "#sess = tf.Session()\n",
    "#K.set_session(sess)\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "embed = hub.Module('https://tfhub.dev/google/elmo/3', trainable=True)\n",
    "\n",
    "class ElmoEmbeddingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.dimensions = 1024\n",
    "        self.trainable=True\n",
    "        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.elmo = hub.Module('https://tfhub.dev/google/elmo/3', trainable=self.trainable,\n",
    "                               name=\"{}_module\".format(self.name))\n",
    "        print(\"self.name : \", self.name)\n",
    "        self.trainable_weights += tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
    "        super(ElmoEmbeddingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        result = self.elmo(tf.squeeze(tf.cast(x, tf.string), axis=1),\n",
    "                      as_dict=True,\n",
    "                      signature='default',\n",
    "                      )['default']\n",
    "        print(\"result.shape : \", result.shape)\n",
    "        return result\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.not_equal(inputs, '--PAD--')\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.dimensions)\n",
    "\n",
    "\n",
    "def ELMoEmbedding(x):\n",
    "  return embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "  input_text = Input(shape=(1,), dtype=tf.string, name = \"Input_01\")\n",
    "  embedding = ElmoEmbeddingLayer()(input_text)\n",
    "  dense = Dense(256, activation='relu', name = \"Dense_01\")(embedding)\n",
    "  pred = Dense(150, activation='sigmoid', name = \"Dense_02\")(dense)\n",
    "  model = Model(inputs=[input_text], outputs=pred)\n",
    "  opt = tf.train.AdamOptimizer(0.01)\n",
    "  model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "def build_model_2(): \n",
    "  input_text = Input(shape=(1,), dtype=tf.string)\n",
    "  embedding = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)\n",
    "  dense = Dense(256, activation='relu')(embedding)\n",
    "  pred = Dense(150, activation='sigmoid')(dense)\n",
    "  model = Model(inputs=[input_text], outputs=pred)\n",
    "  return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "len_train = len(featData[\"Semiclean_Input\"])\n",
    "#Need to clean \"train[label]\" for actual labels\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(featData[\"Semiclean_Input\"], \n",
    "                                                  featData[\"y_output\"].tolist(),  \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1)\n",
    "len_act = len(xtrain)\n",
    "y_sparse_train = featData[\"y_output\"].values.tolist()\n",
    "y_sparse_test = featData[\"y_output\"].values.tolist()[:len_act]\n",
    "\n",
    "train_text = [' '.join(t.split()[0:300]) for t in xtrain]\n",
    "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
    "\n",
    "valid_text = [' '.join(t.split()[0:300]) for t in xvalid]\n",
    "valid_text = np.array(valid_text, dtype=object)[:, np.newaxis]\n",
    "valid_text = K.squeeze(valid_text, axis = 1)\n",
    "\n",
    "ytrain = [ytrain]\n",
    "yvalid = [yvalid]\n",
    "\n",
    "def train_input_fn(batch_size=128):\n",
    "  # Convert the inputs to a Dataset.\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((train_text,ytrain))\n",
    "# Shuffle, repeat, and batch the examples.\n",
    "  dataset = dataset.cache()\n",
    "  dataset = dataset.shuffle(1000, reshuffle_each_iteration=True)\n",
    "  dataset = dataset.repeat()\n",
    "  dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "# Return the dataset.\n",
    "  return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- compiling --\n"
     ]
    }
   ],
   "source": [
    "model = build_model_2()\n",
    "opt = tf.train.AdamOptimizer(0.01)\n",
    "optimizer_cap = tf.contrib.tpu.CrossShardOptimizer(opt)\n",
    "print(\"-- compiling --\")\n",
    "model.compile('sgd', 'mse', metrics = [\"accuracy\"])\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=optimizer_cap, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               38550     \n",
      "=================================================================\n",
      "Total params: 300,950\n",
      "Trainable params: 300,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done one !\n",
      "-- training --\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "16128/45000 [=========>....................] - ETA: 2:02:52 - loss: 0.2490 - acc: 0.0018"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "  #K.set_session(session)\n",
    "  session.run(tf.global_variables_initializer())  \n",
    "  print('done one !')\n",
    "  session.run(tf.tables_initializer())\n",
    "  print(\"-- training --\")\n",
    "  history = model.fit(train_text,\n",
    "        ytrain,\n",
    "        validation_data=(valid_text, yvalid),\n",
    "        #steps_per_epoch = 1,\n",
    "        batch_size = 256,\n",
    "        epochs=5)\n",
    "    #model_elmo.save_weights('./model_elmo_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jjh9yGzs0KMW"
   },
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#To make tf 2.0 compatible with tf1.0 code, we disable the tf2.0 functionalities\n",
    "#tf.disable_eager_execution()\n",
    "import tensorflow_hub as hub\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "#hub.disable_eager_execution()\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "embeddings_train_2 = elmo(\n",
    "    inputs={\n",
    "        \"tokens\": train,\n",
    "        \"sequence_len\": tokens_train_length\n",
    "    },\n",
    "    signature=\"tokens\",\n",
    "    as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "embeddings_train_3 = elmo(\n",
    "    featData[\"Semiclean_Input\"],\n",
    "    signature=\"default\",\n",
    "    as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELMoEmbedding(x):\n",
    "    return elmo(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.compat.v1.get_default_graph()\n",
    "\n",
    "def build_model(): \n",
    "    input_text = Input(shape=(1, ), dtype=\"string\")\n",
    "    embedding = Lambda(ELMoEmbedding, output_shape=(1024, ))(input_text)\n",
    "    dense = Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(embedding)\n",
    "    pred = Dense(150, activation='sigmoid')(dense)\n",
    "    model = Model(inputs=input_text, outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model\n",
    "model_elmo = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(featData[\"Semiclean_Input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34000 samples, validate on 8500 samples\n",
      "Epoch 1/5\n",
      "ERROR:tensorflow:Session failed to close after 30 seconds. Continuing after this point may leave your program in an undefined state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:Session failed to close after 30 seconds. Continuing after this point may leave your program in an undefined state.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-291-e0efbe6e0660>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#sess.run(tf.global_variables_initializer())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#sess.run(tf.tables_initializer())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_elmo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmodel_elmo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model_elmo_weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3550\u001b[0m         \u001b[0mtensor_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3551\u001b[0m         array_vals.append(np.asarray(value,\n\u001b[0;32m-> 3552\u001b[0;31m                                      dtype=tensor_type.as_numpy_dtype))\n\u001b[0m\u001b[1;32m   3553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3554\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "X = featData[\"Semiclean_Input\"][:len_train]\n",
    "y = featData[\"y_output\"][:len_train]\n",
    "\n",
    "#init_op = \n",
    "#init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    \n",
    "    #keras.set_session(session)\n",
    "    #sess.run(tf.global_variables_initializer())  \n",
    "    #sess.run(tf.tables_initializer())\n",
    "    history = model_elmo.fit(X, y, epochs=5, batch_size=256, validation_split = 0.2)\n",
    "    model_elmo.save_weights('./model_elmo_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Untitled",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
